{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing neccessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from datetime import datetime, date\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#from sklearn.cross_validation import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the main train and test files.\n",
    "## We don't need the extras but we're using them for feature generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(\"TRAIN \\n\")\\n\\n\\nprint(df_train.head(5))\\nprint(\"COUNTRIES \\n\")\\nprint(countries.head(5))\\nprint(\"AGE GENDER\\n\")\\n\\n\\nprint(ageGenderBucket.head(5))\\nprint(\"SESSION\\n\")\\n\\n\\nprint(sessions.head(5))'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the Data\n",
    "df_train = pd.read_csv('train_users.csv')\n",
    "df_test = pd.read_csv('test_users.csv')\n",
    "df_train.shape, df_test.shape\n",
    "\n",
    "countries = pd.read_csv('countries.csv')\n",
    "\n",
    "ageGenderBucket = pd.read_csv('age_gender_bkts.csv')\n",
    "\n",
    "sessions = pd.read_csv('sessions.csv')\n",
    "\n",
    "\"\"\"print(\"TRAIN \\n\")\n",
    "\n",
    "\n",
    "print(df_train.head(5))\n",
    "print(\"COUNTRIES \\n\")\n",
    "print(countries.head(5))\n",
    "print(\"AGE GENDER\\n\")\n",
    "\n",
    "\n",
    "print(ageGenderBucket.head(5))\n",
    "print(\"SESSION\\n\")\n",
    "\n",
    "\n",
    "print(sessions.head(5))\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This was testing for our sessions in order to see how to get certain data. We want to apply this to all our data so let's do this after we make it all.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#df_train.where(df_train == \"d1mm9tcy42\")\n",
    "above_35 = df_train[df_train[\"id\"] == \"d1mm9tcy42\"]\n",
    "above_35\n",
    "userIdTest = sessions[sessions[\"user_id\"] == \"d1mm9tcy42\"]\n",
    "print(userIdTest)\n",
    "\n",
    "df_train.country_destination.unique()\n",
    "#print(countries.country_destination == \"NDF\")\n",
    "\n",
    "sessions.action.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_train.country_destination.values\n",
    "id_test = df_test.id\n",
    "df_train.drop(['country_destination'], axis = 1)\n",
    "#Concatenating train and test data for EDA\n",
    "df_all = pd.concat((df_train, df_test), axis = 0, ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Windows Desktop' '-unknown-']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date_account_created</th>\n",
       "      <th>timestamp_first_active</th>\n",
       "      <th>date_first_booking</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>signup_method</th>\n",
       "      <th>signup_flow</th>\n",
       "      <th>language</th>\n",
       "      <th>affiliate_channel</th>\n",
       "      <th>affiliate_provider</th>\n",
       "      <th>first_affiliate_tracked</th>\n",
       "      <th>signup_app</th>\n",
       "      <th>first_device_type</th>\n",
       "      <th>first_browser</th>\n",
       "      <th>country_destination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gxn3p5htnn</td>\n",
       "      <td>2010-06-28</td>\n",
       "      <td>20090319043255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-unknown-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>facebook</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Web</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>820tgsjxq7</td>\n",
       "      <td>2011-05-25</td>\n",
       "      <td>20090523174809</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MALE</td>\n",
       "      <td>38.0</td>\n",
       "      <td>facebook</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>seo</td>\n",
       "      <td>google</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Web</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4ft3gnwmtx</td>\n",
       "      <td>2010-09-28</td>\n",
       "      <td>20090609231247</td>\n",
       "      <td>2010-08-02</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>56.0</td>\n",
       "      <td>basic</td>\n",
       "      <td>3</td>\n",
       "      <td>en</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Web</td>\n",
       "      <td>Windows Desktop</td>\n",
       "      <td>IE</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bjjt8pjhuk</td>\n",
       "      <td>2011-12-05</td>\n",
       "      <td>20091031060129</td>\n",
       "      <td>2012-09-08</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>42.0</td>\n",
       "      <td>facebook</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Web</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87mebub9p4</td>\n",
       "      <td>2010-09-14</td>\n",
       "      <td>20091208061105</td>\n",
       "      <td>2010-02-18</td>\n",
       "      <td>-unknown-</td>\n",
       "      <td>41.0</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Web</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id date_account_created  timestamp_first_active date_first_booking  \\\n",
       "0  gxn3p5htnn           2010-06-28          20090319043255                NaN   \n",
       "1  820tgsjxq7           2011-05-25          20090523174809                NaN   \n",
       "2  4ft3gnwmtx           2010-09-28          20090609231247         2010-08-02   \n",
       "3  bjjt8pjhuk           2011-12-05          20091031060129         2012-09-08   \n",
       "4  87mebub9p4           2010-09-14          20091208061105         2010-02-18   \n",
       "\n",
       "      gender   age signup_method  signup_flow language affiliate_channel  \\\n",
       "0  -unknown-   NaN      facebook            0       en            direct   \n",
       "1       MALE  38.0      facebook            0       en               seo   \n",
       "2     FEMALE  56.0         basic            3       en            direct   \n",
       "3     FEMALE  42.0      facebook            0       en            direct   \n",
       "4  -unknown-  41.0         basic            0       en            direct   \n",
       "\n",
       "  affiliate_provider first_affiliate_tracked signup_app first_device_type  \\\n",
       "0             direct               untracked        Web       Mac Desktop   \n",
       "1             google               untracked        Web       Mac Desktop   \n",
       "2             direct               untracked        Web   Windows Desktop   \n",
       "3             direct               untracked        Web       Mac Desktop   \n",
       "4             direct               untracked        Web       Mac Desktop   \n",
       "\n",
       "  first_browser country_destination  \n",
       "0        Chrome                 NDF  \n",
       "1        Chrome                 NDF  \n",
       "2            IE                  US  \n",
       "3       Firefox               other  \n",
       "4        Chrome                  US  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sessions[sessions.user_id==\"d1mm9tcy42\"].device_type.unique())\n",
    "\n",
    "#print(df_all[df_all.id ==\"d1mm9tcy42\"])\n",
    "\n",
    "above_35 = df_train[df_train[\"id\"] == \"d1mm9tcy42\"]\n",
    "above_35\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country_destination  language_levenshtein_distance\n",
      "0                  AU                           0.00\n",
      "1                  CA                           0.00\n",
      "2                  DE                          72.61\n",
      "3                  ES                          92.25\n",
      "4                  FR                          92.06\n",
      "5                  GB                           0.00\n",
      "6                  IT                          89.40\n",
      "7                  NL                          63.22\n",
      "8                  PT                          95.45\n",
      "9                  US                           0.00\n"
     ]
    }
   ],
   "source": [
    "temp = countries[['country_destination','language_levenshtein_distance']]\n",
    "print(temp)\n",
    "df_all = pd.merge(temp, df_all)\n",
    "#countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>action</th>\n",
       "      <th>action_type</th>\n",
       "      <th>action_detail</th>\n",
       "      <th>device_type</th>\n",
       "      <th>secs_elapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d1mm9tcy42</td>\n",
       "      <td>lookup</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Windows Desktop</td>\n",
       "      <td>319.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d1mm9tcy42</td>\n",
       "      <td>search_results</td>\n",
       "      <td>click</td>\n",
       "      <td>view_search_results</td>\n",
       "      <td>Windows Desktop</td>\n",
       "      <td>67753.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d1mm9tcy42</td>\n",
       "      <td>lookup</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Windows Desktop</td>\n",
       "      <td>301.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d1mm9tcy42</td>\n",
       "      <td>search_results</td>\n",
       "      <td>click</td>\n",
       "      <td>view_search_results</td>\n",
       "      <td>Windows Desktop</td>\n",
       "      <td>22141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d1mm9tcy42</td>\n",
       "      <td>lookup</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Windows Desktop</td>\n",
       "      <td>435.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id          action action_type        action_detail  \\\n",
       "0  d1mm9tcy42          lookup         NaN                  NaN   \n",
       "1  d1mm9tcy42  search_results       click  view_search_results   \n",
       "2  d1mm9tcy42          lookup         NaN                  NaN   \n",
       "3  d1mm9tcy42  search_results       click  view_search_results   \n",
       "4  d1mm9tcy42          lookup         NaN                  NaN   \n",
       "\n",
       "       device_type  secs_elapsed  \n",
       "0  Windows Desktop         319.0  \n",
       "1  Windows Desktop       67753.0  \n",
       "2  Windows Desktop         301.0  \n",
       "3  Windows Desktop       22141.0  \n",
       "4  Windows Desktop         435.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries\n",
    "ageGenderBucket\n",
    "sessions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's work with the sessions data - taken from feature_engineering.py file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sessions\n",
    "df_sessions = sessions\n",
    "df_sessions['id'] = df_sessions['user_id']\n",
    "df_sessions = df_sessions.drop(['user_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sessions.action = df_sessions.action.fillna('NAN')\n",
    "df_sessions.action_type = df_sessions.action_type.fillna('NAN')\n",
    "df_sessions.action_detail = df_sessions.action_detail.fillna('NAN')\n",
    "df_sessions.device_type = df_sessions.device_type.fillna('NAN')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Action values with low frequency are changed to 'OTHER'\n",
    "act_freq = 100  #Threshold for frequency\n",
    "act = dict(zip(*np.unique(df_sessions.action, return_counts=True)))\n",
    "df_sessions.action = df_sessions.action.apply(lambda x: 'OTHER' if act[x] < act_freq else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Computing value_counts. These are going to be used in the one-hot encoding\n",
    "#based feature generation (following loop).\n",
    "f_act = df_sessions.action.value_counts().argsort()\n",
    "f_act_detail = df_sessions.action_detail.value_counts().argsort()\n",
    "f_act_type = df_sessions.action_type.value_counts().argsort()\n",
    "f_dev_type = df_sessions.device_type.value_counts().argsort()\n",
    "\n",
    "#grouping session by id. We will compute features from all rows with the same id.\n",
    "dgr_sess = df_sessions.groupby(['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 from 135483\n",
      "10000 from 135483\n",
      "20000 from 135483\n",
      "30000 from 135483\n",
      "40000 from 135483\n",
      "50000 from 135483\n",
      "60000 from 135483\n",
      "70000 from 135483\n",
      "80000 from 135483\n",
      "90000 from 135483\n",
      "100000 from 135483\n",
      "110000 from 135483\n",
      "120000 from 135483\n",
      "130000 from 135483\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Loop on dgr_sess to create all the features.\n",
    "samples = []\n",
    "cont = 0\n",
    "ln = len(dgr_sess)\n",
    "for g in dgr_sess:\n",
    "    if cont%10000 == 0:\n",
    "        print(\"%s from %s\" %(cont, ln))\n",
    "    gr = g[1]\n",
    "    l = []\n",
    "    \n",
    "    #the id\n",
    "    l.append(g[0])\n",
    "    \n",
    "    #The actual first feature is the number of values.\n",
    "    l.append(len(gr))\n",
    "    \n",
    "    sev = gr.secs_elapsed.fillna(0).values   #These values are used later.\n",
    "    \n",
    "    #action features\n",
    "    #(how many times each value occurs, numb of unique values, mean and std)\n",
    "    c_act = [0] * len(f_act)\n",
    "    for i,v in enumerate(gr.action.values):\n",
    "        c_act[f_act[v]] += 1\n",
    "    _, c_act_uqc = np.unique(gr.action.values, return_counts=True)\n",
    "    c_act += [len(c_act_uqc), np.mean(c_act_uqc), np.std(c_act_uqc)]\n",
    "    l = l + c_act\n",
    "    \n",
    "    #action_detail features\n",
    "    #(how many times each value occurs, numb of unique values, mean and std)\n",
    "    c_act_detail = [0] * len(f_act_detail)\n",
    "    for i,v in enumerate(gr.action_detail.values):\n",
    "        c_act_detail[f_act_detail[v]] += 1 \n",
    "    _, c_act_det_uqc = np.unique(gr.action_detail.values, return_counts=True)\n",
    "    c_act_detail += [len(c_act_det_uqc), np.mean(c_act_det_uqc), np.std(c_act_det_uqc)]\n",
    "    l = l + c_act_detail\n",
    "    \n",
    "    #action_type features\n",
    "    #(how many times each value occurs, numb of unique values, mean and std\n",
    "    #+ log of the sum of secs_elapsed for each value)\n",
    "    l_act_type = [0] * len(f_act_type)\n",
    "    c_act_type = [0] * len(f_act_type)\n",
    "    for i,v in enumerate(gr.action_type.values):\n",
    "        l_act_type[f_act_type[v]] += sev[i]   \n",
    "        c_act_type[f_act_type[v]] += 1  \n",
    "    l_act_type = np.log(1 + np.array(l_act_type)).tolist()\n",
    "    _, c_act_type_uqc = np.unique(gr.action_type.values, return_counts=True)\n",
    "    c_act_type += [len(c_act_type_uqc), np.mean(c_act_type_uqc), np.std(c_act_type_uqc)]\n",
    "    l = l + c_act_type + l_act_type    \n",
    "    \n",
    "    #device_type features\n",
    "    #(how many times each value occurs, numb of unique values, mean and std)\n",
    "    c_dev_type  = [0] * len(f_dev_type)\n",
    "    for i,v in enumerate(gr.device_type .values):\n",
    "        c_dev_type[f_dev_type[v]] += 1 \n",
    "    c_dev_type.append(len(np.unique(gr.device_type.values)))\n",
    "    _, c_dev_type_uqc = np.unique(gr.device_type.values, return_counts=True)\n",
    "    c_dev_type += [len(c_dev_type_uqc), np.mean(c_dev_type_uqc), np.std(c_dev_type_uqc)]        \n",
    "    l = l + c_dev_type    \n",
    "    \n",
    "    #secs_elapsed features        \n",
    "    l_secs = [0] * 5 \n",
    "    l_log = [0] * 15\n",
    "    if len(sev) > 0:\n",
    "        #Simple statistics about the secs_elapsed values.\n",
    "        l_secs[0] = np.log(1 + np.sum(sev))\n",
    "        l_secs[1] = np.log(1 + np.mean(sev)) \n",
    "        l_secs[2] = np.log(1 + np.std(sev))\n",
    "        l_secs[3] = np.log(1 + np.median(sev))\n",
    "        l_secs[4] = l_secs[0] / float(l[1])\n",
    "        \n",
    "        #Values are grouped in 15 intervals. Compute the number of values\n",
    "        #in each interval.\n",
    "        log_sev = np.log(1 + sev).astype(int)\n",
    "        l_log = np.bincount(log_sev, minlength=15).tolist()                      \n",
    "    l = l + l_secs + l_log\n",
    "    \n",
    "    #The list l has the feature values of one sample.\n",
    "    samples.append(l)\n",
    "    cont += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Creating a dataframe with the computed features    \n",
    "col_names = []    #name of the columns\n",
    "for i in range(len(samples[0])-1):\n",
    "    col_names.append('c_' + str(i)) \n",
    "#preparing objects    \n",
    "samples = np.array(samples)\n",
    "samp_ar = samples[:, 1:].astype(np.float16)\n",
    "samp_id = samples[:, 0]   #The first element in obs is the id of the sample.\n",
    "\n",
    "#creating the dataframe        \n",
    "df_agg_sess = pd.DataFrame(samp_ar, columns=col_names)\n",
    "df_agg_sess['id'] = samp_id\n",
    "df_agg_sess.index = df_agg_sess.id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on users data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#########Working on train and test data#####################\n",
    "print('Working on users data...')\n",
    "#Concatenating df_train and df_test\n",
    "df_tt = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n",
    "df_tt.index = df_tt.id\n",
    "df_tt = df_tt.fillna(-1)  #Inputing this kind of missing value with -1 (missing values in train and test)\n",
    "df_tt = df_tt.replace('-unknown-', -1) #-unknown is another way of missing value, then = -1.\n",
    "\n",
    "########Creating features for train+test\n",
    "#Removing date_first_booking\n",
    "df_tt = df_tt.drop(['date_first_booking'], axis=1)\n",
    "\n",
    "#Number of nulls\n",
    "df_tt['n_null'] = np.array([sum(r == -1) for r in df_tt.values])\n",
    "\n",
    "#date_account_created\n",
    "#(Computing year, month, day, week_number, weekday)\n",
    "dac = np.vstack(df_tt.date_account_created.astype(str).apply(lambda x: list(map(int, x.split('-')))).values)\n",
    "df_tt['dac_y'] = dac[:,0]\n",
    "df_tt['dac_m'] = dac[:,1]\n",
    "df_tt['dac_d'] = dac[:,2]\n",
    "dac_dates = [datetime(x[0],x[1],x[2]) for x in dac]\n",
    "df_tt['dac_wn'] = np.array([d.isocalendar()[1] for d in dac_dates])\n",
    "df_tt['dac_w'] = np.array([d.weekday() for d in dac_dates])\n",
    "df_tt_wd = pd.get_dummies(df_tt.dac_w, prefix='dac_w')\n",
    "df_tt = df_tt.drop(['date_account_created', 'dac_w'], axis=1)\n",
    "df_tt = pd.concat((df_tt, df_tt_wd), axis=1)\n",
    "\n",
    "#timestamp_first_active\n",
    "#(Computing year, month, day, hour, week_number, weekday)\n",
    "tfa = np.vstack(df_tt.timestamp_first_active.astype(str).apply(lambda x: list(map(int, [x[:4],x[4:6],x[6:8],x[8:10],x[10:12],x[12:14]]))).values)\n",
    "df_tt['tfa_y'] = tfa[:,0]\n",
    "df_tt['tfa_m'] = tfa[:,1]\n",
    "df_tt['tfa_d'] = tfa[:,2]\n",
    "df_tt['tfa_h'] = tfa[:,3]\n",
    "tfa_dates = [datetime(x[0],x[1],x[2],x[3],x[4],x[5]) for x in tfa]\n",
    "df_tt['tfa_wn'] = np.array([d.isocalendar()[1] for d in tfa_dates])\n",
    "df_tt['tfa_w'] = np.array([d.weekday() for d in tfa_dates])\n",
    "df_tt_wd = pd.get_dummies(df_tt.tfa_w, prefix='tfa_w')\n",
    "df_tt = df_tt.drop(['timestamp_first_active', 'tfa_w'], axis=1)\n",
    "df_tt = pd.concat((df_tt, df_tt_wd), axis=1)\n",
    "\n",
    "#timespans between dates\n",
    "#(Computing absolute number of seconds of difference between dates, sign of the difference)\n",
    "df_tt['dac_tfa_secs'] = np.array([np.log(1+abs((dac_dates[i]-tfa_dates[i]).total_seconds())) for i in range(len(dac_dates))])\n",
    "df_tt['sig_dac_tfa'] = np.array([np.sign((dac_dates[i]-tfa_dates[i]).total_seconds()) for i in range(len(dac_dates))])\n",
    "#    df_tt['dac_tfa_days'] = np.array([np.sign((dac_dates[i]-tfa_dates[i]).days) for i in range(len(dac_dates))])\n",
    "\n",
    "#Comptute seasons from dates\n",
    "#(Computing the season for the two dates)\n",
    "Y = 2000 # dummy leap year to allow input X-02-29 (leap day)\n",
    "seasons = [(0, (date(Y,  1,  1),  date(Y,  3, 20))),  #'winter'\n",
    "           (1, (date(Y,  3, 21),  date(Y,  6, 20))),  #'spring'\n",
    "           (2, (date(Y,  6, 21),  date(Y,  9, 22))),  #'summer'\n",
    "           (3, (date(Y,  9, 23),  date(Y, 12, 20))),  #'autumn'\n",
    "           (0, (date(Y, 12, 21),  date(Y, 12, 31)))]  #'winter'\n",
    "def get_season(dt):\n",
    "    dt = dt.date()\n",
    "    dt = dt.replace(year=Y)\n",
    "    return next(season for season, (start, end) in seasons\n",
    "                if start <= dt <= end)\n",
    "df_tt['season_dac'] = np.array([get_season(dt) for dt in dac_dates])\n",
    "df_tt['season_tfa'] = np.array([get_season(dt) for dt in tfa_dates])\n",
    "#df_all['season_dfb'] = np.array([get_season(dt) for dt in dfb_dates])\n",
    "\n",
    "#Age\n",
    "#(Keeping ages in 14 < age < 99 as OK and grouping others according different kinds of mistakes)\n",
    "av = df_tt.age.values\n",
    "av = np.where(np.logical_and(av<2000, av>1900), 2014-av, av) #This are birthdays instead of age (estimating age by doing 2014 - value)\n",
    "av = np.where(np.logical_and(av<14, av>0), 4, av) #Using specific value=4 for age values below 14\n",
    "av = np.where(np.logical_and(av<2016, av>2010), 9, av) #This is the current year insted of age (using specific value = 9)\n",
    "av = np.where(av > 99, 110, av)  #Using specific value=110 for age values above 99\n",
    "df_tt['age'] = av\n",
    "\n",
    "#AgeRange\n",
    "#(One-hot encoding of the edge according these intervals)\n",
    "interv =  [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 100]\n",
    "def get_interv_value(age):\n",
    "    iv = 20\n",
    "    for i in range(len(interv)):\n",
    "        if age < interv[i]:\n",
    "            iv = i \n",
    "            break\n",
    "    return iv\n",
    "df_tt['age_interv'] = df_tt.age.apply(lambda x: get_interv_value(x))\n",
    "df_tt_ai = pd.get_dummies(df_tt.age_interv, prefix='age_interv')\n",
    "df_tt = df_tt.drop(['age_interv'], axis=1)\n",
    "df_tt = pd.concat((df_tt, df_tt_ai), axis=1)\n",
    "\n",
    "#One-hot-encoding features\n",
    "ohe_feats = ['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser']\n",
    "for f in ohe_feats:\n",
    "    df_tt_dummy = pd.get_dummies(df_tt[f], prefix=f)\n",
    "    df_tt = df_tt.drop([f], axis=1)\n",
    "    df_tt = pd.concat((df_tt, df_tt_dummy), axis=1)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['id'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-e836396438d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_tt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#df_all = pd.merge(df_tt, df_agg_sess, how='left')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4158\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4159\u001b[0m         \"\"\"\n\u001b[0;32m-> 4160\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   4161\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4162\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3876\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3877\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3878\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3880\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3910\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3911\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3912\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3913\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   5274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5275\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5276\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5277\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5278\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['id'] not found in axis\""
     ]
    }
   ],
   "source": [
    "#df_all.head(5)\n",
    "#df_tt.drop(['id'])\n",
    "\n",
    "#df_all = pd.merge(df_tt, df_agg_sess, how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n######Merging train-test with session data#################\\ndf_all = pd.merge(df_tt, df_agg_sess, how='left')\\ndf_all = df_all.drop(['id'], axis=1)\\ndf_all = df_all.fillna(-2)  #Missing features for samples without sesssion data.\\n#All types of null \\ndf_all['all_null'] = np.array([sum(r<0) for r in df_all.values])\\n\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "######Merging train-test with session data#################\n",
    "df_all = pd.merge(df_tt, df_agg_sess, how='left')\n",
    "df_all = df_all.drop(['id'], axis=1)\n",
    "df_all = df_all.fillna(-2)  #Missing features for samples without sesssion data.\n",
    "#All types of null \n",
    "df_all['all_null'] = np.array([sum(r<0) for r in df_all.values])\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######Computing X, y and X_test ################\n",
    "piv_train = len(target) #Marker to split df_all into train + test\n",
    "vals = df_all.values\n",
    "le = LabelEncoder()\n",
    "\n",
    "X = vals[:piv_train]\n",
    "y = le.fit_transform(target.values)\n",
    "X_test = vals[piv_train:]\n",
    "print('Shape X = %s, Shape X_test = %s'%(X.shape, X_test.shape))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the number of null entries in each column.\n",
    "for i in df_all.columns:\n",
    "    ab = df_all[i].isnull().sum()\n",
    "    if ab != 0:\n",
    "        print(i + \" has {} null values.\".format(ab))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.drop(['id','country_destination','date_first_booking'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_all.columns:\n",
    "    ab = df_all[i].isnull().sum()\n",
    "    if ab != 0:\n",
    "        print(i + \" has {} null values.\".format(ab))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.age.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[df_all['age']<15].age = np.nan\n",
    "df_all[df_all['age']>=100].age = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting below columns as categories for plotting in graphs\n",
    "categorical_features = [\n",
    "    'affiliate_channel',\n",
    "    'affiliate_provider',\n",
    "    'first_affiliate_tracked',\n",
    "    'first_browser',\n",
    "    'first_device_type',\n",
    "    'gender',\n",
    "    'language',\n",
    "    'signup_app',\n",
    "    'signup_method',\n",
    "    'signup_flow'\n",
    "]\n",
    "\n",
    "for categorical_feature in categorical_features:\n",
    "    df_all[categorical_feature] = df_all[categorical_feature].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['date_account_created'] = pd.to_datetime(df_all['date_account_created'])\n",
    "df_all['timestamp_first_active'] = pd.to_datetime(df_all['timestamp_first_active'], format='%Y%m%d%H%M%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the gender distribution\n",
    "\n",
    "plt.figure(figsize=(14,8))\n",
    "order1 = df_all['gender'].value_counts().index\n",
    "sns.countplot(data = df_all, x = 'gender', order = order1, color = sns.color_palette()[0])\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Gender Distribution')\n",
    "order2 = df_all['gender'].value_counts()\n",
    "\n",
    "for i in range(order2.shape[0]):\n",
    "    count = order2[i]\n",
    "    strt='{:0.1f}%'.format(100*count / df_all.shape[0])\n",
    "    plt.text(i,count+1000,strt,ha='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding Destination Distribution.\n",
    "\n",
    "plt.figure(figsize=(14,8))\n",
    "order1 = df_train['country_destination'].value_counts().index\n",
    "sns.countplot(data = df_train, x = 'country_destination', order = order1, color = sns.color_palette()[1])\n",
    "plt.xlabel('Destination')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Destination Distribution')\n",
    "order2 = df_train['country_destination'].value_counts()\n",
    "\n",
    "for i in range(order2.shape[0]):\n",
    "    count = order2[i]\n",
    "    strt='{:0.1f}%'.format(100*count / df_train.shape[0])\n",
    "    plt.text(i,count+1000,strt,ha='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting year for date_account_created\n",
    "df_all['acc_year'] = df_all['date_account_created'].dt.year\n",
    "\n",
    "\n",
    "df1 = df_all.groupby('acc_year').count()\n",
    "df1.head()\n",
    "\n",
    "years = [2010,2011,2012,2013,2014]\n",
    "yearsOrder=pd.api.types.CategoricalDtype(ordered=True, categories=years)\n",
    "df1.reset_index(inplace = True)\n",
    "df1.acc_year = df1.acc_year.astype(yearsOrder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding year-wise distribution\n",
    "\n",
    "plt.figure(figsize=[14,8])\n",
    "sns.barplot(data=df1,x='acc_year',y='affiliate_provider',color=sns.color_palette()[2]);\n",
    "plt.title('Year wise distribution');\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Counts')\n",
    "for i in range(df1.shape[0]):\n",
    "    count=df1.iloc[i]['affiliate_provider']\n",
    "    strt='{:0.2f}%'.format(100*count/df_all.shape[0])\n",
    "    plt.text(i,count+1000,strt,ha='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a month centered dataframe for 2014\n",
    "df2 = df_all[df_all['date_account_created'].dt.year==2014]\n",
    "df2['monthYear14'] = df2['date_account_created'].map(lambda x: x.strftime('%m-%Y'))\n",
    "df2 = df2.groupby('monthYear14').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of accounts created in different month of 2014\n",
    "\n",
    "plt.figure(figsize=[14,8])\n",
    "sns.barplot(data=df2,x=df2.index,y='affiliate_provider',color=sns.color_palette()[3]);\n",
    "plt.title('2014 month wise distribution');\n",
    "plt.xlabel('Month-Year')\n",
    "plt.ylabel('Counts')\n",
    "for i in range(df2.shape[0]):\n",
    "    count=df2.iloc[i]['affiliate_provider']\n",
    "    strt='{:0.2f}%'.format(100*count/df_all.shape[0])\n",
    "    plt.text(i,count+100,strt,ha='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing member ages into various bins\n",
    "\n",
    "df_all['member_age_bins']=df_all['age'].apply(lambda x: '18 - 20' if 18<x<=20\n",
    "                                                  else '20 - 30' if 20<x<=30\n",
    "                                                  else '30 - 40' if 30<x<=40\n",
    "                                                  else '40 - 50' if 40<x<=50\n",
    "                                                  else '50 - 60' if 50<x<=60\n",
    "                                                  else '60-70' if 60<x<=70\n",
    "                                                  else '70+' if 70<x<=100\n",
    "                                                  else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a month centered dataframe for 2014\n",
    "\n",
    "df3 = df_all[df_all['date_account_created'].dt.year==2013]\n",
    "df3['monthYear13'] = df3['date_account_created'].map(lambda x: x.strftime('%m-%Y'))\n",
    "df3 = df3.groupby('monthYear13').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of accounts created in different month of 2014\n",
    "\n",
    "plt.figure(figsize=[14,8])\n",
    "sns.barplot(data=df3,x=df3.index,y='affiliate_provider',color=sns.color_palette()[4]);\n",
    "plt.title('2013 month wise distribution');\n",
    "plt.xlabel('Month-Year')\n",
    "plt.ylabel('Counts')\n",
    "for i in range(df3.shape[0]):\n",
    "    count=df3.iloc[i]['affiliate_provider']\n",
    "    strt='{:0.2f}%'.format(100*count/df_all.shape[0])\n",
    "    plt.text(i,count+100,strt,ha='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Age distribution of the members\n",
    "plt.figure(figsize=[14,8])\n",
    "sns.distplot(df_all.age.dropna(),bins=np.arange(18,100+5,5),color=sns.color_palette()[5],kde=False);\n",
    "plt.xlabel('Age of members')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Age distribution of Users')\n",
    "plt.xlim(18,100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bivariate Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Destination-Age distribution plot\n",
    "\n",
    "plt.figure(figsize=[14,8])\n",
    "sns.boxplot(data=df_train,y='age',x='country_destination',color=sns.color_palette()[6]);\n",
    "plt.ylim(18,100)\n",
    "plt.xlabel('Country');\n",
    "plt.ylabel('Age');\n",
    "plt.title('Country-Age Distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gender-Age Distribution plot\n",
    "\n",
    "plt.figure(figsize=[14,8])\n",
    "sns.boxplot(data=df_all,y='age',x='gender',color=sns.color_palette()[7]);\n",
    "plt.ylim(18,100)\n",
    "plt.xlabel('Gender');\n",
    "plt.ylabel('Age');\n",
    "plt.title('Gender-Age Distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Gender-Destination Distribution Plot\n",
    "\n",
    "plt.figure(figsize=(14,8))\n",
    "order1 = df_train['country_destination'].value_counts().index\n",
    "sns.countplot(data = df_train, x = 'country_destination', order = order1,hue='gender')\n",
    "plt.xlabel('Destination')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Gender-Destination Distribution')\n",
    "order2 = df_train['country_destination'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df_all[df_all['date_account_created'].dt.year==2013]\n",
    "df3['monthYear13'] = df3['date_account_created'].map(lambda x: x.strftime('%m-%Y'))\n",
    "\n",
    "df3= df3.groupby(['monthYear13','member_age_bins']).count()\n",
    "\n",
    "df3.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age wise distribution of the booking in different months of 2013\n",
    "\n",
    "plt.figure(figsize=[14,8])\n",
    "sns.pointplot(data=df3,x='monthYear13',y='affiliate_provider',hue='member_age_bins');\n",
    "plt.title('2013 month-age wise distribution');\n",
    "plt.xlabel('Month-Year')\n",
    "plt.ylabel('Counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df_all[df_all['date_account_created'].dt.year==2013]\n",
    "df3['monthYear13'] = df3['date_account_created'].map(lambda x: x.strftime('%m-%Y'))\n",
    "\n",
    "df3= df3.groupby(['monthYear13','gender']).count()\n",
    "\n",
    "df3.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gender wise distribution of the booking in different months of 2013\n",
    "\n",
    "plt.figure(figsize=[14,8])\n",
    "sns.pointplot(data=df3,x='monthYear13',y='affiliate_provider',hue='gender');\n",
    "plt.title('2013 month-gender wise distribution');\n",
    "plt.xlabel('Month-Year')\n",
    "plt.ylabel('Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relation ship between age-gender-destination\n",
    "\n",
    "plt.figure(figsize=[14,8])\n",
    "sns.boxplot(data=df_train,y='age',x='country_destination',hue='gender');\n",
    "plt.ylim(18,100)\n",
    "plt.xlabel('Country');\n",
    "plt.ylabel('Age');\n",
    "plt.title('Country-Age Distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation for the Booking Destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['country_destination'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWork = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n",
    "dfWork = dfWork.drop(['id', 'date_first_booking'], axis=1)\n",
    "#Filling nan\n",
    "dfWork = dfWork.fillna(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date_account_created\n",
    "\n",
    "dac = np.vstack(dfWork.date_account_created.astype(str).apply(lambda x: list(map(int, x.split('-')))).values)\n",
    "dfWork['dac_year'] = dac[:,0]\n",
    "dfWork['dac_month'] = dac[:,1]\n",
    "dfWork['dac_day'] = dac[:,2]\n",
    "dfWork = dfWork.drop(['date_account_created'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timestamp_first_active\n",
    "\n",
    "tfa = np.vstack(dfWork.timestamp_first_active.astype(str).apply(lambda x: list(map(int, [x[:4],x[4:6],x[6:8],x[8:10],x[10:12],x[12:14]]))).values)\n",
    "dfWork['tfa_year'] = tfa[:,0]\n",
    "dfWork['tfa_month'] = tfa[:,1]\n",
    "dfWork['tfa_day'] = tfa[:,2]\n",
    "dfWork = dfWork.drop(['timestamp_first_active'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av = dfWork.age.values\n",
    "dfWork['age'] = np.where(np.logical_or(av<14, av>100), -1, av)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot-encoding features\n",
    "ohe_feats = ['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel',\n",
    "             'affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser'] \n",
    "for f in ohe_feats:\n",
    "    dfWork_dummy = pd.get_dummies(dfWork[f], prefix=f)\n",
    "    dfWork = dfWork.drop([f], axis=1)\n",
    "    dfWork = pd.concat((dfWork, dfWork_dummy), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting train and test\n",
    "vals = dfWork.values\n",
    "X = vals[:df_train.shape[0]]\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)   \n",
    "X_test = vals[df_train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier\n",
    "xgb = XGBClassifier(max_depth=6, learning_rate=0.3, n_estimators=25,\n",
    "                    objective='multi:softprob', subsample=0.5, colsample_bytree=0.5, seed=0)                  \n",
    "xgb.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction\n",
    "y_pred = xgb.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking the 5 classes with highest probabilities\n",
    "ids = []  #list of ids\n",
    "cts = []  #list of countries\n",
    "for i in range(len(id_test)):\n",
    "    idx = id_test[i]\n",
    "    ids += [idx] * 5\n",
    "    cts += le.inverse_transform(np.argsort(y_pred[i])[::-1])[:5].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate submission\n",
    "sub = pd.DataFrame(np.column_stack((ids, cts)), columns=['id', 'country'])\n",
    "sub.to_csv('sub.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
